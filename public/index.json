[{"content":"\nWhy This Matters: The Shift to Agentic AI The software landscape is shifting toward agentic AI systems - applications where Large Language Models (LLMs) don\u0026rsquo;t just answer questions, but actively use tools to solve complex problems. Instead of building separate AI features, the market and industry are moving toward AI that can directly interact with your existing systems, databases, and workflows.\nThis creates a fundamental challenge: how do you expose your application\u0026rsquo;s capabilities to an LLM? How do you let Claude, for instance, query your database, analyze the results or trigger your business logic - all through natural conversation?\nEnter the Model Context Protocol (MCP), open-sourced by Anthropic in November 2024. MCP provides a standardized way to connect any application to Claude (or other LLMs) through well-defined tools and protocols. Think of it as a bridge that lets your applications participate in AI conversations by exposing your functionality as tools.\nThis post walks through building a complete MCP integration - from a local FastAPI application with PostgreSQL (a personal \u0026ldquo;goal tracking\u0026rdquo; app) to a conversational interface where you can ask Claude to query your database, analyze trends, and even update records. You\u0026rsquo;ll see the practical patterns, common gotchas in setup, and why this approach represents a fundamental shift in how we build intelligent applications.\nThe goal isn\u0026rsquo;t just to add AI features to your app - it\u0026rsquo;s to make your entire application conversational and intelligent by design.\nThe Magic of MCP: Apps Meet Conversational AI Building conversational AI interfaces has been unlocked thanks to Anthropic\u0026rsquo;s Model Context Protocol (MCP). In this post, I\u0026rsquo;ll walk you through creating a local FastAPI application with PostgreSQL that Claude can interact with directly through structured tools - your local app talking to Claude Desktop. This demonstrates the core MCP integration patterns, and because we\u0026rsquo;re running everything locally, we won\u0026rsquo;t get bogged down with API keys, authentication, or hosting complexities.\nImagine asking Claude: \u0026ldquo;What goals do I have set up?\u0026rdquo; and having it query your PostgreSQL database, return structured data, and provide intelligent insights - all through a simple conversation. That\u0026rsquo;s a surface-level glimpse of the power of MCP.\nMCP bridges the gap between your applications and Claude\u0026rsquo;s conversational abilities by providing:\nStructured tool definitions that Claude can discover and use Type-safe data exchange through JSON schemas Real-time conversations with your actual application data Architecture Overview The stack here demonstrates a modern Python development pattern:\n1 2 3 Claude Desktop ↔ MCP Server ↔ FastAPI App ↔ PostgreSQL ↕ Pydantic Models (Type Safety) Key Components: Pydantic Models: Define data structure and automatic validation SQLAlchemy: ORM for PostgreSQL with relationship management FastAPI: REST API with auto-generated OpenAPI docs MCP Server: Bridge between Claude and your application PostgreSQL: Robust relational database with JSON support Project Structure 1 2 3 4 5 6 7 8 9 src/ ├── core/ │ ├── models/ # Pydantic data models │ ├── database/ # SQLAlchemy models and connection │ ├── api/ # FastAPI routes and app │ └── mcp/ # MCP server integration │ ├── server.py # Main MCP server │ ├── tools.py # Tool definitions for Claude │ └── handlers.py # Tool implementation logic Step 1: Pydantic Models for Type Safety Pydantic models serve as the single source of truth for your data structure:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 from pydantic import BaseModel, Field from typing import List, Optional from uuid import UUID from datetime import datetime class Goal(BaseModel): \u0026#34;\u0026#34;\u0026#34;A specific, time-bound outcome to achieve.\u0026#34;\u0026#34;\u0026#34; id: UUID = Field(default_factory=uuid4) title: str = Field(..., description=\u0026#34;Clear, specific goal statement\u0026#34;) progress_percentage: float = Field(default=0.0, ge=0.0, le=100.0) status: str = Field(default=\u0026#34;active\u0026#34;) created_at: datetime = Field(default_factory=datetime.utcnow) class GoalWithMetrics(Goal): \u0026#34;\u0026#34;\u0026#34;Goal with nested metrics for rich responses.\u0026#34;\u0026#34;\u0026#34; metrics: List[Metric] = Field(default_factory=list) Why this matters: These same models generate:\nDatabase schemas (via SQLAlchemy) API documentation (via FastAPI) MCP tool schemas (for Claude integration) Step 2: SQLAlchemy Database Models Map Pydantic models to database tables. This is the database layer that mirrors your Pydantic models. You\u0026rsquo;re defining the actual PostgreSQL table structure using SQLAlchemy\u0026rsquo;s ORM syntax.\nThe key pattern here is model mapping - your Goal Pydantic model becomes a GoalTable SQLAlchemy model with the same fields, but now with database-specific details like column types, constraints, and relationships.\nThe relationship(\u0026quot;MetricTable\u0026quot;, back_populates=\u0026quot;goal\u0026quot;) sets up the foreign key relationship so you can easily query goals with their associated metrics in one go.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 from sqlalchemy import Column, String, Float, DateTime from sqlalchemy.dialects.postgresql import UUID as PGUUID class GoalTable(Base): __tablename__ = \u0026#34;goals\u0026#34; id = Column(PGUUID(as_uuid=True), primary_key=True, default=uuid4) title = Column(String(200), nullable=False) progress_percentage = Column(Float, default=0.0) status = Column(String(20), default=\u0026#34;active\u0026#34;) created_at = Column(DateTime(timezone=True), server_default=func.now()) # Relationships for complex queries metrics = relationship(\u0026#34;MetricTable\u0026#34;, back_populates=\u0026#34;goal\u0026#34;) Why this matters: You now have type-safe data models (Pydantic) that automatically generate both your API schemas and your database tables (SQLAlchemy), keeping everything in sync without manual duplication.\nDatabase migrations with Alembic handle schema evolution:\n1 2 3 4 5 # Generate migration from model changes poetry run alembic revision --autogenerate -m \u0026#34;Add goals table\u0026#34; # Apply migrations poetry run alembic upgrade head Step 3: FastAPI Routes Create REST endpoints that return Pydantic models:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 from fastapi import FastAPI, Depends from sqlalchemy.orm import Session app = FastAPI(title=\u0026#34;Your Local App\u0026#34;) @app.get(\u0026#34;/goals\u0026#34;, response_model=List[GoalWithMetrics]) async def get_goals(session: Session = Depends(get_session)): goals = session.query(GoalTable).all() # Convert SQLAlchemy to Pydantic with nested data return [ GoalWithMetrics.model_validate({ \u0026#34;id\u0026#34;: goal.id, \u0026#34;title\u0026#34;: goal.title, \u0026#34;progress_percentage\u0026#34;: goal.progress_percentage, \u0026#34;status\u0026#34;: goal.status, \u0026#34;created_at\u0026#34;: goal.created_at, \u0026#34;metrics\u0026#34;: [metric_to_dict(m) for m in goal.metrics] }) for goal in goals ] Step 4: MCP Tool Definitions Now we get to the fun part - defining tools that Claude can discover and use!\nHere we are creating a \u0026ldquo;menu\u0026rdquo; of tools that tells Claude exactly what it can use from your application. Each tool definition is like a menu item that specifies:\nThe name - get_goals What it does - \u0026ldquo;Get goals with their current progress\u0026rdquo; What options are available - status_filter, include_metrics The inputSchema is the detailed specification of those options - it tells Claude \u0026ldquo;you can filter by active, completed, or paused, and you can choose whether to include metrics (default is yes).\u0026rdquo;\nWhen Claude sees this menu, it knows exactly how to \u0026ldquo;order\u0026rdquo; from your app: it can ask for goals, specify which status it wants, and decide whether it needs the extra metric details. This is the contract between Claude and your application - Claude knows what\u0026rsquo;s possible, and your app knows what to expect:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 from mcp.types import Tool def get_goals_tool() -\u0026gt; Tool: \u0026#34;\u0026#34;\u0026#34;Tool for Claude to get goals with progress.\u0026#34;\u0026#34;\u0026#34; return Tool( name=\u0026#34;get_goals\u0026#34;, description=\u0026#34;Get goals with their current progress and metrics\u0026#34;, inputSchema={ \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;status_filter\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;enum\u0026#34;: [\u0026#34;active\u0026#34;, \u0026#34;completed\u0026#34;, \u0026#34;paused\u0026#34;], \u0026#34;description\u0026#34;: \u0026#34;Filter goals by status\u0026#34; }, \u0026#34;include_metrics\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;boolean\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Whether to include metric details\u0026#34;, \u0026#34;default\u0026#34;: True } } } ) Tip: Use Pydantic.model_json_schema() to auto-generate complex schemas:\n1 2 3 4 5 6 def create_goal_tool() -\u0026gt; Tool: return Tool( name=\u0026#34;create_goal\u0026#34;, description=\u0026#34;Create a new goal\u0026#34;, inputSchema=Goal.model_json_schema() # Automatic schema generation! ) Step 5: MCP Server Implementation Create the server that bridges Claude and your app.\nThe MCP server has two main jobs:\n\u0026ldquo;Here\u0026rsquo;s what I can do\u0026rdquo; - When Claude asks \u0026ldquo;what tools are available?\u0026rdquo;, the list_tools() function responds with a menu: \u0026ldquo;I can get goals, create goals, and update goals.\u0026rdquo;\n\u0026ldquo;Let me do that for you\u0026rdquo; - When Claude says \u0026ldquo;please get my goals with status=active\u0026rdquo;, the call_tool() function:\nFigures out which specific function to call (handle_get_goals) Passes along Claude\u0026rsquo;s parameters ({\u0026quot;status\u0026quot;: \u0026quot;active\u0026quot;}) Runs your database code Sends the results back to Claude as text The run_mcp_server() function starts up this \u0026ldquo;translator\u0026rdquo; and keeps it running, listening for Claude\u0026rsquo;s requests through standard input/output (like a command-line program).\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 from mcp.server import Server from mcp.types import TextContent from mcp.server.stdio import stdio_server def create_server(): server = Server(\u0026#34;your-app-name\u0026#34;) @server.list_tools() async def list_tools(): return [get_goals_tool(), create_goal_tool(), update_goal_tool()] @server.call_tool() async def call_tool(name: str, arguments: dict): try: if name == \u0026#34;get_goals\u0026#34;: result = await handle_get_goals(arguments) elif name == \u0026#34;create_goal\u0026#34;: result = await handle_create_goal(arguments) # ... other tools return [TextContent(type=\u0026#34;text\u0026#34;, text=result)] except Exception as e: # Error handling - shows up in Claude logs print(f\u0026#34;Error in tool {name}: {str(e)}\u0026#34;, file=sys.stderr) return [TextContent(type=\u0026#34;text\u0026#34;, text=f\u0026#34;Error: {str(e)}\u0026#34;)] return server async def run_mcp_server(): server = create_server() async with stdio_server() as (read_stream, write_stream): await server.run(read_stream, write_stream, server.create_initialization_options()) Step 6: Tool Implementation with Database Integration Here we are creating the bridge between MCP and the database by:\nExtracting parameters from Claude\u0026rsquo;s tool call (like filtering and options) Querying your database using standard SQLAlchemy patterns with optional filtering Converting SQLAlchemy objects to plain dictionaries for JSON serialization Optionally loading related data (metrics) based on the include_metrics flag Returning structured JSON that Claude can understand and work with Essentially, you\u0026rsquo;re translating between Claude\u0026rsquo;s conversational requests and your database\u0026rsquo;s structured data - taking natural language tool calls and turning them into database queries, then formatting the results back into something Claude can interpret and discuss with the user.\nThe pattern is: MCP tool call → database query → JSON response\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 async def handle_get_goals(arguments: dict) -\u0026gt; str: status_filter = arguments.get(\u0026#34;status_filter\u0026#34;) include_metrics = arguments.get(\u0026#34;include_metrics\u0026#34;, True) # Use your existing database session session = next(get_session()) try: query = session.query(GoalTable) if status_filter: query = query.filter(GoalTable.status == status_filter) goals = query.order_by(GoalTable.created_at).all() # Convert to Pydantic for validation and serialization goal_objects = [] for goal in goals: goal_data = { \u0026#34;id\u0026#34;: str(goal.id), \u0026#34;title\u0026#34;: goal.title, \u0026#34;progress_percentage\u0026#34;: goal.progress_percentage, \u0026#34;status\u0026#34;: goal.status, \u0026#34;metrics\u0026#34;: [] } if include_metrics: metrics = session.query(MetricTable).filter( MetricTable.goal_id == goal.id ).all() goal_data[\u0026#34;metrics\u0026#34;] = [metric_to_dict(m) for m in metrics] goal_objects.append(goal_data) # Return formatted JSON for Claude return json.dumps(goal_objects, indent=2) finally: session.close() Step 7: Claude Desktop Configuration Configure Claude Desktop to connect to your MCP server:\nThis step instructs Claude Desktop on where to locate your MCP server. You\u0026rsquo;re adding an entry to Claude\u0026rsquo;s configuration file that specifies:\nWhat to call - the command to start your MCP server Where to run it - the working directory for your project How to identify it - a name Claude uses to reference this server Once configured, Claude Desktop will automatically start your MCP server when it launches and connect to it for tool access. The result: Claude can now discover and use the tools you defined in previous steps.\nFile: ~/Library/Application Support/Claude/claude_desktop_config.json\n1 2 3 4 5 6 7 8 9 { \u0026#34;mcpServers\u0026#34;: { \u0026#34;your-app-name\u0026#34;: { \u0026#34;command\u0026#34;: \u0026#34;/bin/bash\u0026#34;, \u0026#34;args\u0026#34;: [\u0026#34;-c\u0026#34;, \u0026#34;cd /path/to/your/project \u0026amp;\u0026amp; poetry run python -m src.core.mcp.server\u0026#34;], \u0026#34;cwd\u0026#34;: \u0026#34;/path/to/your/project\u0026#34; } } } This follows the official MCP configuration pattern documented by Anthropic. Key points:\nUse absolute paths for reliability Ensure proper working directory with cwd Use bash wrapper to handle environment setup Note: In a production environment, you\u0026rsquo;d typically have your own AI agent or application connecting to the MCP server. Here, Claude Desktop acts as our \u0026ldquo;agent\u0026rdquo; - it discovers your MCP server on startup and makes the tools available through the chat interface. This local setup enables you to quickly explore MCP integrations before integrating them into larger systems.\nBONUS: CLI Integration for Easy Management Create a CLI for managing your servers:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 import typer import uvicorn import asyncio app = typer.Typer() @app.command() def start_api(): \u0026#34;\u0026#34;\u0026#34;Start the FastAPI server.\u0026#34;\u0026#34;\u0026#34; uvicorn.run(\u0026#34;src.core.api.app:app\u0026#34;, host=\u0026#34;127.0.0.1\u0026#34;, port=8000, reload=True) @app.command() def start_mcp(): \u0026#34;\u0026#34;\u0026#34;Start the MCP server for Claude.\u0026#34;\u0026#34;\u0026#34; from .mcp.server import run_mcp_server asyncio.run(run_mcp_server()) @app.command() def migrate(): \u0026#34;\u0026#34;\u0026#34;Run database migrations.\u0026#34;\u0026#34;\u0026#34; subprocess.run([\u0026#34;alembic\u0026#34;, \u0026#34;upgrade\u0026#34;, \u0026#34;head\u0026#34;]) Usage:\n1 2 3 poetry run your-app start-api # FastAPI server poetry run your-app start-mcp # MCP server poetry run your-app migrate # Database migrations The Result: Conversational Database Interactions Once everything is connected, you can have natural conversations with your data:\nYou: \u0026ldquo;What are my 3 most recent goals?\u0026rdquo;\nClaude: Looking at your goals, your 3 most recent goals appear to be:\n[Calls get_goals with status_filter=\u0026ldquo;active\u0026rdquo;, include_metrics=true]\nClaude calls the get_goals tool and then analyzes the results to identify the 3 most recently created goals.\nClaude: *Based on your current goals, I found 3 that might need attention.\nYou could enhance your tool definition to include:\nlimit parameter (number of results to return) sort_by parameter (\u0026ldquo;created_at\u0026rdquo;, \u0026ldquo;updated_at\u0026rdquo;, etc.) That would make the call: [Calls get_goals with limit=3, sort_by=\u0026ldquo;created_at\u0026rdquo;, include_metrics=true]\nAdvanced Patterns Rich Insights with Cross-Domain Analysis Leverage Claude\u0026rsquo;s analytical capabilities:\nInstead of asking Claude to run individual queries (\u0026ldquo;show me my goals\u0026rdquo;, \u0026ldquo;show me my metrics\u0026rdquo;), you\u0026rsquo;re giving it rich, interconnected data and letting it find patterns and insights you might miss.\nThe analytical leap:\nRaw approach: \u0026ldquo;What are my goals?\u0026rdquo; → Claude returns a list Rich approach: \u0026ldquo;Here\u0026rsquo;s my goals, metrics, and recent activity - what insights do you see?\u0026rdquo; → Claude identifies trends, correlations, and recommendations Concrete example: Claude might notice: \u0026ldquo;Your deadlift goal is at 15% progress, but I see you haven\u0026rsquo;t logged any gym sessions in 3 weeks, and your recent check-ins show you\u0026rsquo;ve been deep in learning new technologies. These might be connected - your intense focus on skill development may be crowding out your strength training routine.\u0026rdquo;\nWhy this is powerful:\nCross-domain thinking - Claude can spot relationships between different life areas Pattern recognition - Finds trends across time and different data types Proactive insights - Suggests what to focus on rather than just reporting status Strategic recommendations - Not just \u0026ldquo;what happened\u0026rdquo; but \u0026ldquo;what should I do next\u0026rdquo; By connecting the goals app to the LLM, you\u0026rsquo;ve essentially transformed a logging and metrics app into an AI-powered personal strategist that can see the whole picture and help you make better decisions about where to focus your energy.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 async def handle_progress_summary(arguments: dict) -\u0026gt; str: # Gather data from multiple tables goals = get_goals_data() metrics = get_metrics_data() recent_activity = get_recent_checkins() # Return rich data for Claude to analyze return json.dumps({ \u0026#34;goals\u0026#34;: goals, \u0026#34;metrics\u0026#34;: metrics, \u0026#34;recent_activity\u0026#34;: recent_activity, \u0026#34;suggested_insights\u0026#34;: [ \u0026#34;Cross-reference goal progress with activity patterns\u0026#34;, \u0026#34;Identify goals that haven\u0026#39;t been updated recently\u0026#34;, \u0026#34;Find correlation between metrics and goal completion\u0026#34; ] }) Multiple MCP Servers You can run multiple MCP servers for different domains:\nComposability - you can connect multiple specialized servers to Claude simultaneously. Each server can focus on a different domain (your main app, analytics, financial data, etc.), and Claude can use tools from all of them in a single conversation.\nInstead of building one monolithic MCP server, you can create focused, single-purpose servers that Claude orchestrates together. Ask Claude to \u0026ldquo;analyze my goals progress and compare it with my financial metrics\u0026rdquo; - it seamlessly pulls from multiple apps to give you cross-system insights.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 { \u0026#34;mcpServers\u0026#34;: { \u0026#34;main-app\u0026#34;: { \u0026#34;command\u0026#34;: \u0026#34;poetry\u0026#34;, \u0026#34;args\u0026#34;: [\u0026#34;run\u0026#34;, \u0026#34;main-app\u0026#34;, \u0026#34;start-mcp\u0026#34;], \u0026#34;cwd\u0026#34;: \u0026#34;/path/to/main/app\u0026#34; }, \u0026#34;analytics-app\u0026#34;: { \u0026#34;command\u0026#34;: \u0026#34;poetry\u0026#34;, \u0026#34;args\u0026#34;: [\u0026#34;run\u0026#34;, \u0026#34;analytics\u0026#34;, \u0026#34;start-mcp\u0026#34;], \u0026#34;cwd\u0026#34;: \u0026#34;/path/to/analytics/app\u0026#34; } } } Streaming Responses for Large Data For large datasets, consider streaming responses:\n1 2 3 4 5 6 7 8 9 10 11 12 13 async def handle_large_query(arguments: dict) -\u0026gt; str: # Process in chunks results = [] for chunk in process_in_batches(query_data(arguments)): results.extend(chunk) if len(results) \u0026gt; 1000: # Limit response size break return json.dumps({ \u0026#34;results\u0026#34;: results, \u0026#34;truncated\u0026#34;: len(results) \u0026gt;= 1000, \u0026#34;total_available\u0026#34;: get_total_count(arguments) }) Common Issues and Solutions 1. \u0026ldquo;Poetry could not find pyproject.toml\u0026rdquo; Error: MCP server can\u0026rsquo;t find your project files.\nSolution: Configuration probably has the wrong directory. Use shell wrapper with explicit directory change:\n1 2 3 4 { \u0026#34;command\u0026#34;: \u0026#34;/bin/bash\u0026#34;, \u0026#34;args\u0026#34;: [\u0026#34;-c\u0026#34;, \u0026#34;cd /full/path/to/project \u0026amp;\u0026amp; poetry run your-command\u0026#34;] } 2. \u0026ldquo;relation \u0026rsquo;table_name\u0026rsquo; does not exist\u0026rdquo; Error: Database tables not created.\nSolution: You likely forgot to run a migration - run migrations before starting MCP server:\n1 poetry run alembic upgrade head 3. JSON Parsing Errors in Claude Logs Error: Unexpected non-whitespace character after JSON\nCause: SQLAlchemy debug output mixing with JSON responses.\nSolution: Either disable SQLAlchemy echo or ignore these warnings:\n1 engine = create_engine(DATABASE_URL, echo=False) # Disable in production 4. Module Import Warnings Warning: RuntimeWarning: \u0026lsquo;module\u0026rsquo; found in sys.modules\nSolution: This warning is harmless when running MCP servers. It occurs due to Python\u0026rsquo;s module loading order but doesn\u0026rsquo;t affect functionality.\nDebugging MCP Connections View MCP logs:\n1 tail -f ~/Library/Logs/Claude/mcp-server-your-app-name.log Test MCP server manually:\n1 2 3 cd /your/project/path poetry run python -m src.core.mcp.server # Should wait for input without errors Add debug output to MCP handlers:\n1 2 3 4 5 6 7 8 9 10 async def handle_tool(arguments: dict) -\u0026gt; str: try: # Your logic here result = process_data(arguments) return json.dumps(result) except Exception as e: # This appears in Claude logs print(f\u0026#34;Debug: Tool failed with {arguments}\u0026#34;, file=sys.stderr) print(f\u0026#34;Error: {str(e)}\u0026#34;, file=sys.stderr) return f\u0026#34;Error: {str(e)}\u0026#34; Conclusion Building MCP-enabled applications opens up imaginative possibilities for AI interactions and augmentation - transforming static applications into intelligent partners that can analyze, strategize, and evolve with your needs. The combination of Pydantic\u0026rsquo;s type safety, FastAPI\u0026rsquo;s performance, PostgreSQL\u0026rsquo;s robustness, and MCP\u0026rsquo;s conversational interface creates a powerful foundation for intelligent applications.\nKey takeaways: Pydantic models provide type safety across your entire stack MCP integration is surprisingly straightforward once you understand the stdio communication pattern Error handling and logging are crucial for debugging MCP connections The development experience is smooth with proper tooling and CLI commands Standardized AI integration - MCP provides the protocol layer that lets any LLM interact with your applications through well-defined tools The result is a system that enables natural conversations with your data, allows you to create complex queries through simple language, and provides intelligent insights.\nNext steps: The patterns demonstrated here extend naturally to production environments with proper authentication, advanced tool chaining, and rich user interfaces. The future of AI application development leverages LLM advances to unlock the full potential of our applications, transforming them from static data repositories with APIs into dynamic, intelligent partners.\nBuilt with: Python 3.12, FastAPI, PostgreSQL, Pydantic, SQLAlchemy, Alembic, and Anthropic\u0026rsquo;s MCP\nAbout the Author: Mark Holton is a hands-on Software Architect at ShiftUp, where we\u0026rsquo;re building AI agents that revolutionize Go To Market and Sales Intelligence.\nOriginally published to Medium: https://markholton.medium.com/ai-integration-building-conversational-apps-with-mcp-f47a487009f5\n","permalink":"http://localhost:59970/posts/mcp_blog_post/","summary":"\u003cp\u003e\u003cimg alt=\"AI Integration with Data Pipelines\" loading=\"lazy\" src=\"/images/building_pipelines.jpg\"\u003e\u003c/p\u003e\n\u003ch2 id=\"why-this-matters-the-shift-to-agentic-ai\"\u003eWhy This Matters: The Shift to Agentic AI\u003c/h2\u003e\n\u003cp\u003eThe software landscape is shifting toward agentic AI systems - applications where Large Language Models (LLMs) don\u0026rsquo;t just answer questions, but actively use tools to solve complex problems. Instead of building separate AI features, the market and industry are moving toward AI that can directly interact with your existing systems, databases, and workflows.\u003c/p\u003e\n\u003cp\u003eThis creates a fundamental challenge: how do you expose your application\u0026rsquo;s capabilities to an LLM? How do you let Claude, for instance, query your database, analyze the results or trigger your business logic - all through natural conversation?\u003c/p\u003e","title":"AI Integration: Building Conversational Apps with MCP"},{"content":"About Mark Holton I\u0026rsquo;m a hands-on Software Architect with over two decades of experience building resilient, scalable systems. I specialize in data platform engineering, distributed architectures, and agent-driven systems that solve real-world problems.\nCurrently, I\u0026rsquo;m architecting data pipelines for GoToMarket Sales Intelligence at ShiftUpAI.com. Before that, I spent a decade building systems at Salesforce. Here I share what I\u0026rsquo;m learning about modern architectures, AI integration patterns, and navigating the next phase of a software engineering career.\nWhat I Do I spend my days designing and building:\nData Pipelines \u0026amp; Platforms - Scalable systems that turn raw data into actionable insights Distributed Systems - Backend architectures that handle complexity gracefully Agent-Driven Solutions - AI-integrated systems that enhance human capabilities Resilient Architectures - Systems that fail gracefully and recover quickly Philosophy After two decades in software engineering, I\u0026rsquo;ve learned that the best architectures are like good stories - they have clear structure, handle complexity gracefully, and actually solve real problems. I believe in building both systems and life intentionally.\nPublications I\u0026rsquo;ve written about data platform engineering and fault-tolerant system design:\nBuilding a Fault-Tolerant Data Pipeline for Chatbots - Salesforce Engineering Blog Building a Scalable Event Pipeline with Heroku and Salesforce - Salesforce Engineering Blog Patents Signals and Measurement of Business Goals in a Chatbot Platform - US Patent 17/932,750, Filed Sep 16, 2022, Granted Oct 8, 2024 Connect With Me I share insights about software architecture, career transitions, and lessons learned from the trenches of engineering.\nlinkedin.com/in/markholtonsoftware\nx.com/markholton\nThis blog is where I document my journey through the evolving landscape of software engineering, data platforms, and the art of building systems that matter.\n","permalink":"http://localhost:59970/about/","summary":"\u003ch1 id=\"about-mark-holton\"\u003eAbout Mark Holton\u003c/h1\u003e\n\u003cp\u003e\u003cimg alt=\"Data Engineer\" loading=\"lazy\" src=\"/images/data_engineer.jpg\"\u003e\u003c/p\u003e\n\u003cp\u003eI\u0026rsquo;m a hands-on Software Architect with over two decades of experience building resilient, scalable systems. I specialize in data platform engineering, distributed architectures, and agent-driven systems that solve real-world problems.\u003c/p\u003e\n\u003cp\u003eCurrently, I\u0026rsquo;m architecting data pipelines for GoToMarket Sales Intelligence at \u003ca href=\"https://shiftupai.com\"\u003eShiftUpAI.com\u003c/a\u003e. Before that, I spent a decade building systems at \u003ca href=\"salesforce.com\"\u003eSalesforce\u003c/a\u003e. Here I share what I\u0026rsquo;m learning about modern architectures, AI integration patterns, and navigating the next phase of a software engineering career.\u003c/p\u003e","title":"About"},{"content":"Data Pipelines and Possibilities! I\u0026rsquo;m a Software and Data Architect focused on building resilient, agent-driven data platforms and data pipelines.\nAfter two decades of building software systems, I\u0026rsquo;ve learned that the best architectures are like good stories - they have a clear structure, handle complexity gracefully, and actually solve real problems. I spend my days designing data pipelines, distributed systems, and backend architectures that scale. Here I share insights from the trenches of software engineering, lessons learned from career transitions, and discoveries about building both systems and life intentionally.\nWhat to Expect (When I can) I\u0026rsquo;ll be writing about:\nSoftware architecture and design patterns Data platform engineering Agent-driven systems and AI integration Resilient system design Technology trends and best practices Getting Started This blog is built with:\nHugo - The world\u0026rsquo;s fastest framework for building websites PaperMod - A fast, clean, responsive Hugo theme GitHub Pages - Free hosting for static sites Stay tuned for more content about building robust, scalable data platforms!\n","permalink":"http://localhost:59970/posts/hello-world/","summary":"\u003ch1 id=\"data-pipelines-and-possibilities\"\u003eData Pipelines and Possibilities!\u003c/h1\u003e\n\u003cp\u003e\u003cimg alt=\"Data Engineer\" loading=\"lazy\" src=\"/images/data_engineer.jpg\"\u003e\u003c/p\u003e\n\u003cp\u003eI\u0026rsquo;m a Software and Data Architect focused on building resilient, agent-driven data platforms and data pipelines.\u003c/p\u003e\n\u003cp\u003eAfter two decades of building software systems, I\u0026rsquo;ve learned that the best architectures are like good stories - they have a clear structure, handle complexity gracefully, and actually solve real problems. I spend my days designing data pipelines, distributed systems, and backend architectures that scale. Here I share insights from the trenches of software engineering, lessons learned from career transitions, and discoveries about building both systems and life intentionally.\u003c/p\u003e","title":"On the board with Hugo"}]